{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkUUkdZk66cMBRoqx892Gg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditikutwal/Internship/blob/main/WebscrapingAssign_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9zMMv42qjRg"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Write a python program to display all the header tags from wikipedia.org and make data frame."
      ],
      "metadata": {
        "id": "rGByv24sq1eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
        "page"
      ],
      "metadata": {
        "id": "t8uiMgj6rOWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(page.content, 'html.parser')"
      ],
      "metadata": {
        "id": "1h5VNywnVvbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header_texts = []\n",
        "\n",
        "for i in soup.find_all('span', class_=\"mw-headline\"):       #find_all because we want all titles\n",
        "      header_texts.append(i.text)                           #i.text as we want only the text part\n",
        "\n",
        "header_texts\n"
      ],
      "metadata": {
        "id": "eUabFYW9Y-Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'Header': header_texts})\n",
        "df"
      ],
      "metadata": {
        "id": "BHP3gGRQXrt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
      ],
      "metadata": {
        "id": "hMiJZyz_bUKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page = requests.get('https://presidentofindia.nic.in/former-presidents')\n",
        "page"
      ],
      "metadata": {
        "id": "enah9gYsbXKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(page.content, 'html.parser')"
      ],
      "metadata": {
        "id": "Hu16vYIgcyPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = []\n",
        "\n",
        "for i in soup.find_all('div', class_=\"desc-sec\"):\n",
        "  names.append(i.text.replace('\\n', ' '))\n",
        "\n",
        "names"
      ],
      "metadata": {
        "id": "b43mqxexc_pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above question, I'm not able to split the names and the tenure of the former presidents."
      ],
      "metadata": {
        "id": "IFc_ArhgccTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'Former Presidents': names})\n",
        "df"
      ],
      "metadata": {
        "id": "-5un-TIid47W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
        "a) Top 10 ODI teams in menâ€™s cricket along with the records for matches, points and rating.\n"
      ],
      "metadata": {
        "id": "2uNGS1yD-2UJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page = requests.get('https://www.icc-cricket.com/rankings/team-rankings/mens/odi')\n",
        "page"
      ],
      "metadata": {
        "id": "oLDqD2tX-6Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "teams = []\n",
        "table = soup.find('table', class_='table')\n",
        "rows = table.find_all('tr')\n",
        "\n",
        "for row in rows[1:11]:\n",
        "  cells = row.find_all('td')\n",
        "  team = cells[1].text.strip()\n",
        "  matches = cells[2].text.strip()\n",
        "  points = cells[3].text.strip()\n",
        "  rating = cells[4].text.strip()\n",
        "  teams.append([team, matches, points, rating])\n",
        "\n",
        "df = pd.DataFrame({'Team':teams, 'Matches': matches, 'Points': points, 'Rating':ratings})\n",
        "df"
      ],
      "metadata": {
        "id": "zbY6JM_N6-8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm not able to extract the table in this question and the later question as well."
      ],
      "metadata": {
        "id": "v-ZzxgTOcqlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame\n",
        "i) Headline\n",
        "ii) Time\n",
        "iii) News Link"
      ],
      "metadata": {
        "id": "lDqva5ZQRiqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page = requests.get('https://www.cnbc.com/world/?region=world')\n",
        "page"
      ],
      "metadata": {
        "id": "-RInTuYV8W5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(page.content, 'html.parser')"
      ],
      "metadata": {
        "id": "3lqIZMVrR6Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headline = []\n",
        "\n",
        "for i in soup.find_all('div', class_ = 'RiverHeadline-headline RiverHeadline-hasThumbnail'):\n",
        "  headline.append(i.text)\n",
        "\n",
        "headline"
      ],
      "metadata": {
        "id": "8zlc5W0USH8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time = []\n",
        "\n",
        "for i in soup.find_all('span', class_ = 'RiverByline-datePublished'):\n",
        "  time.append(i.text)\n",
        "\n",
        "time"
      ],
      "metadata": {
        "id": "2_na3xtASp23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "FtMSSE520ZCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = []\n",
        "\n",
        "for i in soup.find_all('a', attrs = {'href': re.compile(\"^https://\")}):\n",
        "  link.append(i.href)\n",
        "\n",
        "link"
      ],
      "metadata": {
        "id": "KPhD7suKUFs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm having the same doubt in this question and the questions after it as I'm not able to scrape the links for the titles."
      ],
      "metadata": {
        "id": "X3wu1CvZdGhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kindly clear my doubts regarding the stated doubts."
      ],
      "metadata": {
        "id": "yk8SUO5Bdt8g"
      }
    }
  ]
}